# This defines the rules on how to build parts of the wordfreq lists, using the
# Ninja build system:
#
#   http://martine.github.io/ninja/manual.html
#
# Ninja is available in the 'ninja-build' Ubuntu package. It's like make with
# better parallelism and the ability for build steps to produce multiple
# outputs. The tradeoff is that its rule syntax isn't full of magic for
# expanding wildcards and finding dependencies, so in general you have to
# write the dependencies using a script.
#
# This file will become the header of the larger build.ninja file, which also
# contains the programatically-defined dependency graph.


rule download
  command = python setup.py nltk_download && mkdir -p $prefix && curl -o $out $url

rule extract_tar
  command = tar -xvf $in -C $prefix

rule extract_wiktionary
  command = mkdir -p $$(dirname $out) && python -m conceptnet5.readers.extract_wiktionary $in $$(dirname $out) -l $lang && cd conceptnet5/wiktparse && eval "make ${lang}_parser.py"

## STEP 2: Parse the data
rule parse
  command = mkdir -p $$(dirname $out) && python -m conceptnet5.readers.$parser $in $out

rule parse_sw
  command = mkdir -p $$(dirname $out) && mkdir -p $prefix && python -m conceptnet5.readers.$parser $dir $out

rule parse_wiktionary
  command =  python -m conceptnet5.readers.wiktionary $data $out -l $lang

rule msgpack_to_csv
  command = python -m conceptnet5.builders.msgpack_to_csv $in $out

#TODO improve name
rule collate
  command = cat $in | python -m conceptnet5.builders.distribute_edges -o $$(dirname $out) -n $count

rule count_and_rank
  command = mkdir -p $$(dirname $out) && LC_ALL=C sort $in | uniq > $out

rule combine_assertions
  command = mkdir -p $$(dirname $out) && $in $out -l /l/CC/By-SA

rule build_db
  command = python -m conceptnet5.builders.index_assertions $$(dirname $in) $out --input-shards 8
